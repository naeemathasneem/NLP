{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1KjNWT272HAnxpJ0EsHz2GtzfmgrIn_bM","authorship_tag":"ABX9TyPvP75XauUFj80bhzYOaQeS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"iD8b7e-QoFle"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"UU8xCBzpiOH6","executionInfo":{"status":"ok","timestamp":1717997822309,"user_tz":-330,"elapsed":1551,"user":{"displayName":"naimu samad","userId":"06085924539137588202"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","from sklearn import metrics"]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/softroniics/ML/hyperparameter tuning/Copy of diabetes (1) (2) (1) (1) (1).csv')"],"metadata":{"id":"3wsJseusiVR8","executionInfo":{"status":"ok","timestamp":1717997823886,"user_tz":-330,"elapsed":1580,"user":{"displayName":"naimu samad","userId":"06085924539137588202"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"Vjmeih53iixg","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1717997823886,"user_tz":-330,"elapsed":33,"user":{"displayName":"naimu samad","userId":"06085924539137588202"}},"outputId":"3526344e-bbbd-4bb3-c0b8-bc6aa404ccf3"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n","0            6      148             72             35        0  33.6   \n","1            1       85             66             29        0  26.6   \n","2            8      183             64              0        0  23.3   \n","3            1       89             66             23       94  28.1   \n","4            0      137             40             35      168  43.1   \n","\n","   DiabetesPedigreeFunction  Age  Outcome  \n","0                     0.627   50        1  \n","1                     0.351   31        0  \n","2                     0.672   32        1  \n","3                     0.167   21        0  \n","4                     2.288   33        1  "],"text/html":["\n","  <div id=\"df-d265aa04-6a1b-46ab-80b0-d3fd3af79155\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pregnancies</th>\n","      <th>Glucose</th>\n","      <th>BloodPressure</th>\n","      <th>SkinThickness</th>\n","      <th>Insulin</th>\n","      <th>BMI</th>\n","      <th>DiabetesPedigreeFunction</th>\n","      <th>Age</th>\n","      <th>Outcome</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6</td>\n","      <td>148</td>\n","      <td>72</td>\n","      <td>35</td>\n","      <td>0</td>\n","      <td>33.6</td>\n","      <td>0.627</td>\n","      <td>50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>85</td>\n","      <td>66</td>\n","      <td>29</td>\n","      <td>0</td>\n","      <td>26.6</td>\n","      <td>0.351</td>\n","      <td>31</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8</td>\n","      <td>183</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>23.3</td>\n","      <td>0.672</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>89</td>\n","      <td>66</td>\n","      <td>23</td>\n","      <td>94</td>\n","      <td>28.1</td>\n","      <td>0.167</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>137</td>\n","      <td>40</td>\n","      <td>35</td>\n","      <td>168</td>\n","      <td>43.1</td>\n","      <td>2.288</td>\n","      <td>33</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d265aa04-6a1b-46ab-80b0-d3fd3af79155')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d265aa04-6a1b-46ab-80b0-d3fd3af79155 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d265aa04-6a1b-46ab-80b0-d3fd3af79155');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-faa9cdf8-3e71-47a9-8f55-038c5364ee48\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-faa9cdf8-3e71-47a9-8f55-038c5364ee48')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-faa9cdf8-3e71-47a9-8f55-038c5364ee48 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 768,\n  \"fields\": [\n    {\n      \"column\": \"Pregnancies\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 17,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          6,\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Glucose\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 31,\n        \"min\": 0,\n        \"max\": 199,\n        \"num_unique_values\": 136,\n        \"samples\": [\n          151,\n          101,\n          112\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BloodPressure\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19,\n        \"min\": 0,\n        \"max\": 122,\n        \"num_unique_values\": 47,\n        \"samples\": [\n          86,\n          46,\n          85\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SkinThickness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 51,\n        \"samples\": [\n          7,\n          12,\n          48\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Insulin\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 115,\n        \"min\": 0,\n        \"max\": 846,\n        \"num_unique_values\": 186,\n        \"samples\": [\n          52,\n          41,\n          183\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BMI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.884160320375446,\n        \"min\": 0.0,\n        \"max\": 67.1,\n        \"num_unique_values\": 248,\n        \"samples\": [\n          19.9,\n          31.0,\n          38.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DiabetesPedigreeFunction\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3313285950127749,\n        \"min\": 0.078,\n        \"max\": 2.42,\n        \"num_unique_values\": 517,\n        \"samples\": [\n          1.731,\n          0.426,\n          0.138\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 21,\n        \"max\": 81,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          60,\n          47,\n          72\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Outcome\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["x = df.drop('Outcome', axis=1)\n","y = df['Outcome']\n","\n"],"metadata":{"id":"nc5IDXH9SjOL","executionInfo":{"status":"ok","timestamp":1717997823886,"user_tz":-330,"elapsed":30,"user":{"displayName":"naimu samad","userId":"06085924539137588202"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)"],"metadata":{"id":"ZBp5uHkEUhpz","executionInfo":{"status":"ok","timestamp":1717997823886,"user_tz":-330,"elapsed":29,"user":{"displayName":"naimu samad","userId":"06085924539137588202"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","cls = KNeighborsClassifier(n_neighbors=5)\n","cls.fit(x_train, y_train)\n","y_pred = cls.predict(x_test)\n","y_pred"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NYyqDVE4U4jB","executionInfo":{"status":"ok","timestamp":1717997823886,"user_tz":-330,"elapsed":29,"user":{"displayName":"naimu samad","userId":"06085924539137588202"}},"outputId":"cd857a19-25da-4176-dca8-69769b4a5a0c"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n","       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n","       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n","       1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n","       0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n","       0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n","       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","accuracy_score(y_test, y_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y1O4zkSXVWpi","executionInfo":{"status":"ok","timestamp":1717997823886,"user_tz":-330,"elapsed":27,"user":{"displayName":"naimu samad","userId":"06085924539137588202"}},"outputId":"15df32d6-0a2d-4144-f1b5-be115b5a381c"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7552083333333334"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["help(cls)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zw_gnSaVVcTj","executionInfo":{"status":"ok","timestamp":1717997823887,"user_tz":-330,"elapsed":27,"user":{"displayName":"naimu samad","userId":"06085924539137588202"}},"outputId":"78931105-4f16-40bd-e538-084444ced094"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on KNeighborsClassifier in module sklearn.neighbors._classification object:\n","\n","class KNeighborsClassifier(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.ClassifierMixin, sklearn.neighbors._base.NeighborsBase)\n"," |  KNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n"," |  \n"," |  Classifier implementing the k-nearest neighbors vote.\n"," |  \n"," |  Read more in the :ref:`User Guide <classification>`.\n"," |  \n"," |  Parameters\n"," |  ----------\n"," |  n_neighbors : int, default=5\n"," |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n"," |  \n"," |  weights : {'uniform', 'distance'}, callable or None, default='uniform'\n"," |      Weight function used in prediction.  Possible values:\n"," |  \n"," |      - 'uniform' : uniform weights.  All points in each neighborhood\n"," |        are weighted equally.\n"," |      - 'distance' : weight points by the inverse of their distance.\n"," |        in this case, closer neighbors of a query point will have a\n"," |        greater influence than neighbors which are further away.\n"," |      - [callable] : a user-defined function which accepts an\n"," |        array of distances, and returns an array of the same shape\n"," |        containing the weights.\n"," |  \n"," |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n"," |      Algorithm used to compute the nearest neighbors:\n"," |  \n"," |      - 'ball_tree' will use :class:`BallTree`\n"," |      - 'kd_tree' will use :class:`KDTree`\n"," |      - 'brute' will use a brute-force search.\n"," |      - 'auto' will attempt to decide the most appropriate algorithm\n"," |        based on the values passed to :meth:`fit` method.\n"," |  \n"," |      Note: fitting on sparse input will override the setting of\n"," |      this parameter, using brute force.\n"," |  \n"," |  leaf_size : int, default=30\n"," |      Leaf size passed to BallTree or KDTree.  This can affect the\n"," |      speed of the construction and query, as well as the memory\n"," |      required to store the tree.  The optimal value depends on the\n"," |      nature of the problem.\n"," |  \n"," |  p : int, default=2\n"," |      Power parameter for the Minkowski metric. When p = 1, this is\n"," |      equivalent to using manhattan_distance (l1), and euclidean_distance\n"," |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n"," |  \n"," |  metric : str or callable, default='minkowski'\n"," |      Metric to use for distance computation. Default is \"minkowski\", which\n"," |      results in the standard Euclidean distance when p = 2. See the\n"," |      documentation of `scipy.spatial.distance\n"," |      <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n"," |      the metrics listed in\n"," |      :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n"," |      values.\n"," |  \n"," |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n"," |      must be square during fit. X may be a :term:`sparse graph`, in which\n"," |      case only \"nonzero\" elements may be considered neighbors.\n"," |  \n"," |      If metric is a callable function, it takes two arrays representing 1D\n"," |      vectors as inputs and must return one value indicating the distance\n"," |      between those vectors. This works for Scipy's metrics, but is less\n"," |      efficient than passing the metric name as a string.\n"," |  \n"," |  metric_params : dict, default=None\n"," |      Additional keyword arguments for the metric function.\n"," |  \n"," |  n_jobs : int, default=None\n"," |      The number of parallel jobs to run for neighbors search.\n"," |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n"," |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n"," |      for more details.\n"," |      Doesn't affect :meth:`fit` method.\n"," |  \n"," |  Attributes\n"," |  ----------\n"," |  classes_ : array of shape (n_classes,)\n"," |      Class labels known to the classifier\n"," |  \n"," |  effective_metric_ : str or callble\n"," |      The distance metric used. It will be same as the `metric` parameter\n"," |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n"," |      'minkowski' and `p` parameter set to 2.\n"," |  \n"," |  effective_metric_params_ : dict\n"," |      Additional keyword arguments for the metric function. For most metrics\n"," |      will be same with `metric_params` parameter, but may also contain the\n"," |      `p` parameter value if the `effective_metric_` attribute is set to\n"," |      'minkowski'.\n"," |  \n"," |  n_features_in_ : int\n"," |      Number of features seen during :term:`fit`.\n"," |  \n"," |      .. versionadded:: 0.24\n"," |  \n"," |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n"," |      Names of features seen during :term:`fit`. Defined only when `X`\n"," |      has feature names that are all strings.\n"," |  \n"," |      .. versionadded:: 1.0\n"," |  \n"," |  n_samples_fit_ : int\n"," |      Number of samples in the fitted data.\n"," |  \n"," |  outputs_2d_ : bool\n"," |      False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n"," |      otherwise True.\n"," |  \n"," |  See Also\n"," |  --------\n"," |  RadiusNeighborsClassifier: Classifier based on neighbors within a fixed radius.\n"," |  KNeighborsRegressor: Regression based on k-nearest neighbors.\n"," |  RadiusNeighborsRegressor: Regression based on neighbors within a fixed radius.\n"," |  NearestNeighbors: Unsupervised learner for implementing neighbor searches.\n"," |  \n"," |  Notes\n"," |  -----\n"," |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n"," |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n"," |  \n"," |  .. warning::\n"," |  \n"," |     Regarding the Nearest Neighbors algorithms, if it is found that two\n"," |     neighbors, neighbor `k+1` and `k`, have identical distances\n"," |     but different labels, the results will depend on the ordering of the\n"," |     training data.\n"," |  \n"," |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n"," |  \n"," |  Examples\n"," |  --------\n"," |  >>> X = [[0], [1], [2], [3]]\n"," |  >>> y = [0, 0, 1, 1]\n"," |  >>> from sklearn.neighbors import KNeighborsClassifier\n"," |  >>> neigh = KNeighborsClassifier(n_neighbors=3)\n"," |  >>> neigh.fit(X, y)\n"," |  KNeighborsClassifier(...)\n"," |  >>> print(neigh.predict([[1.1]]))\n"," |  [0]\n"," |  >>> print(neigh.predict_proba([[0.9]]))\n"," |  [[0.666... 0.333...]]\n"," |  \n"," |  Method resolution order:\n"," |      KNeighborsClassifier\n"," |      sklearn.neighbors._base.KNeighborsMixin\n"," |      sklearn.base.ClassifierMixin\n"," |      sklearn.neighbors._base.NeighborsBase\n"," |      sklearn.base.MultiOutputMixin\n"," |      sklearn.base.BaseEstimator\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __init__(self, n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  fit(self, X, y)\n"," |      Fit the k-nearest neighbors classifier from the training dataset.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n"," |          Training data.\n"," |      \n"," |      y : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n"," |          Target values.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : KNeighborsClassifier\n"," |          The fitted k-nearest neighbors classifier.\n"," |  \n"," |  predict(self, X)\n"," |      Predict the class labels for the provided data.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n"," |          Test samples.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      y : ndarray of shape (n_queries,) or (n_queries, n_outputs)\n"," |          Class labels for each data sample.\n"," |  \n"," |  predict_proba(self, X)\n"," |      Return probability estimates for the test data X.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n"," |          Test samples.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      p : ndarray of shape (n_queries, n_classes), or a list of n_outputs                 of such arrays if n_outputs > 1.\n"," |          The class probabilities of the input samples. Classes are ordered\n"," |          by lexicographic order.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes defined here:\n"," |  \n"," |  __abstractmethods__ = frozenset()\n"," |  \n"," |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n"," |  \n"," |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n"," |      Find the K-neighbors of a point.\n"," |      \n"," |      Returns indices of and distances to the neighbors of each point.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None\n"," |          The query point or points.\n"," |          If not provided, neighbors of each indexed point are returned.\n"," |          In this case, the query point is not considered its own neighbor.\n"," |      \n"," |      n_neighbors : int, default=None\n"," |          Number of neighbors required for each sample. The default is the\n"," |          value passed to the constructor.\n"," |      \n"," |      return_distance : bool, default=True\n"," |          Whether or not to return the distances.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      neigh_dist : ndarray of shape (n_queries, n_neighbors)\n"," |          Array representing the lengths to points, only present if\n"," |          return_distance=True.\n"," |      \n"," |      neigh_ind : ndarray of shape (n_queries, n_neighbors)\n"," |          Indices of the nearest points in the population matrix.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      In the following example, we construct a NearestNeighbors\n"," |      class from an array representing our data set and ask who's\n"," |      the closest point to [1,1,1]\n"," |      \n"," |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n"," |      >>> from sklearn.neighbors import NearestNeighbors\n"," |      >>> neigh = NearestNeighbors(n_neighbors=1)\n"," |      >>> neigh.fit(samples)\n"," |      NearestNeighbors(n_neighbors=1)\n"," |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n"," |      (array([[0.5]]), array([[2]]))\n"," |      \n"," |      As you can see, it returns [[0.5]], and [[2]], which means that the\n"," |      element is at distance 0.5 and is the third element of samples\n"," |      (indexes start at 0). You can also query for multiple points:\n"," |      \n"," |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n"," |      >>> neigh.kneighbors(X, return_distance=False)\n"," |      array([[1],\n"," |             [2]]...)\n"," |  \n"," |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n"," |      Compute the (weighted) graph of k-Neighbors for points in X.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None\n"," |          The query point or points.\n"," |          If not provided, neighbors of each indexed point are returned.\n"," |          In this case, the query point is not considered its own neighbor.\n"," |          For ``metric='precomputed'`` the shape should be\n"," |          (n_queries, n_indexed). Otherwise the shape should be\n"," |          (n_queries, n_features).\n"," |      \n"," |      n_neighbors : int, default=None\n"," |          Number of neighbors for each sample. The default is the value\n"," |          passed to the constructor.\n"," |      \n"," |      mode : {'connectivity', 'distance'}, default='connectivity'\n"," |          Type of returned matrix: 'connectivity' will return the\n"," |          connectivity matrix with ones and zeros, in 'distance' the\n"," |          edges are distances between points, type of distance\n"," |          depends on the selected metric parameter in\n"," |          NearestNeighbors class.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n"," |          `n_samples_fit` is the number of samples in the fitted data.\n"," |          `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n"," |          The matrix is of CSR format.\n"," |      \n"," |      See Also\n"," |      --------\n"," |      NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n"," |          of Neighbors for points in X.\n"," |      \n"," |      Examples\n"," |      --------\n"," |      >>> X = [[0], [3], [1]]\n"," |      >>> from sklearn.neighbors import NearestNeighbors\n"," |      >>> neigh = NearestNeighbors(n_neighbors=2)\n"," |      >>> neigh.fit(X)\n"," |      NearestNeighbors(n_neighbors=2)\n"," |      >>> A = neigh.kneighbors_graph(X)\n"," |      >>> A.toarray()\n"," |      array([[1., 0., 1.],\n"," |             [0., 1., 1.],\n"," |             [1., 0., 1.]])\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from sklearn.neighbors._base.KNeighborsMixin:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.ClassifierMixin:\n"," |  \n"," |  score(self, X, y, sample_weight=None)\n"," |      Return the mean accuracy on the given test data and labels.\n"," |      \n"," |      In multi-label classification, this is the subset accuracy\n"," |      which is a harsh metric since you require for each sample that\n"," |      each label set be correctly predicted.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : array-like of shape (n_samples, n_features)\n"," |          Test samples.\n"," |      \n"," |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n"," |          True labels for `X`.\n"," |      \n"," |      sample_weight : array-like of shape (n_samples,), default=None\n"," |          Sample weights.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      score : float\n"," |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.BaseEstimator:\n"," |  \n"," |  __getstate__(self)\n"," |  \n"," |  __repr__(self, N_CHAR_MAX=700)\n"," |      Return repr(self).\n"," |  \n"," |  __setstate__(self, state)\n"," |  \n"," |  get_params(self, deep=True)\n"," |      Get parameters for this estimator.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      deep : bool, default=True\n"," |          If True, will return the parameters for this estimator and\n"," |          contained subobjects that are estimators.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      params : dict\n"," |          Parameter names mapped to their values.\n"," |  \n"," |  set_params(self, **params)\n"," |      Set the parameters of this estimator.\n"," |      \n"," |      The method works on simple estimators as well as on nested objects\n"," |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n"," |      parameters of the form ``<component>__<parameter>`` so that it's\n"," |      possible to update each component of a nested object.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      **params : dict\n"," |          Estimator parameters.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : estimator instance\n"," |          Estimator instance.\n","\n"]}]},{"cell_type":"code","source":["cls1 = KNeighborsClassifier()"],"metadata":{"id":"aDccKVAeVfhV","executionInfo":{"status":"ok","timestamp":1717997823887,"user_tz":-330,"elapsed":24,"user":{"displayName":"naimu samad","userId":"06085924539137588202"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["#GridSearchCV"],"metadata":{"id":"5CQM4DYdaQqw"}},{"cell_type":"code","source":["params ={'n_neighbors':[3,5,7,9,11],'weights':['uniform','distance']}\n","clf = GridSearchCV(cls1,params,cv=10 ,scoring = 'accuracy')\n","clf.fit(x_train,y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":117},"id":"ZD1AYp5SaMmu","executionInfo":{"status":"ok","timestamp":1717997823887,"user_tz":-330,"elapsed":24,"user":{"displayName":"naimu samad","userId":"06085924539137588202"}},"outputId":"c71a6d75-8325-4faa-c883-5abc7d834f88"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=10, estimator=KNeighborsClassifier(),\n","             param_grid={'n_neighbors': [3, 5, 7, 9, 11],\n","                         'weights': ['uniform', 'distance']},\n","             scoring='accuracy')"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10, estimator=KNeighborsClassifier(),\n","             param_grid={&#x27;n_neighbors&#x27;: [3, 5, 7, 9, 11],\n","                         &#x27;weights&#x27;: [&#x27;uniform&#x27;, &#x27;distance&#x27;]},\n","             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10, estimator=KNeighborsClassifier(),\n","             param_grid={&#x27;n_neighbors&#x27;: [3, 5, 7, 9, 11],\n","                         &#x27;weights&#x27;: [&#x27;uniform&#x27;, &#x27;distance&#x27;]},\n","             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["clf.best_params_"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UM0ZF5d-cGHo","executionInfo":{"status":"ok","timestamp":1717997823887,"user_tz":-330,"elapsed":22,"user":{"displayName":"naimu samad","userId":"06085924539137588202"}},"outputId":"c8b5956f-f20c-49fe-8abf-9b2438238c54"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'n_neighbors': 9, 'weights': 'uniform'}"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["clf2 = KNeighborsClassifier(n_neighbors=9, weights='uniform')\n","clf2.fit(x_train, y_train)\n","y_pred = clf2.predict(x_test)\n","accuracy_score(y_test, y_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jbdimps-cOLG","executionInfo":{"status":"ok","timestamp":1717997823887,"user_tz":-330,"elapsed":17,"user":{"displayName":"naimu samad","userId":"06085924539137588202"}},"outputId":"9dc6347b-6a2d-44b9-e56e-757c2e426d7c"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7708333333333334"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["#RandomSearchCV"],"metadata":{"id":"21OwH4EJfhAp"}},{"cell_type":"code","source":["from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from scipy.stats import randint as sp_randit"],"metadata":{"id":"vMsy3fRccnpz","executionInfo":{"status":"ok","timestamp":1717997825387,"user_tz":-330,"elapsed":1516,"user":{"displayName":"naimu samad","userId":"06085924539137588202"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["clf = RandomForestClassifier(n_estimators = 50)"],"metadata":{"id":"qrfHkOg8jCxa","executionInfo":{"status":"ok","timestamp":1717997825387,"user_tz":-330,"elapsed":18,"user":{"displayName":"naimu samad","userId":"06085924539137588202"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["help(clf)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1SrqS6CcjbBC","executionInfo":{"status":"ok","timestamp":1717997825387,"user_tz":-330,"elapsed":17,"user":{"displayName":"naimu samad","userId":"06085924539137588202"}},"outputId":"b51f21c3-c6ff-4070-e2ac-c7febcbf1479"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on RandomForestClassifier in module sklearn.ensemble._forest object:\n","\n","class RandomForestClassifier(ForestClassifier)\n"," |  RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n"," |  \n"," |  A random forest classifier.\n"," |  \n"," |  A random forest is a meta estimator that fits a number of decision tree\n"," |  classifiers on various sub-samples of the dataset and uses averaging to\n"," |  improve the predictive accuracy and control over-fitting.\n"," |  The sub-sample size is controlled with the `max_samples` parameter if\n"," |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n"," |  each tree.\n"," |  \n"," |  Read more in the :ref:`User Guide <forest>`.\n"," |  \n"," |  Parameters\n"," |  ----------\n"," |  n_estimators : int, default=100\n"," |      The number of trees in the forest.\n"," |  \n"," |      .. versionchanged:: 0.22\n"," |         The default value of ``n_estimators`` changed from 10 to 100\n"," |         in 0.22.\n"," |  \n"," |  criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n"," |      The function to measure the quality of a split. Supported criteria are\n"," |      \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n"," |      Shannon information gain, see :ref:`tree_mathematical_formulation`.\n"," |      Note: This parameter is tree-specific.\n"," |  \n"," |  max_depth : int, default=None\n"," |      The maximum depth of the tree. If None, then nodes are expanded until\n"," |      all leaves are pure or until all leaves contain less than\n"," |      min_samples_split samples.\n"," |  \n"," |  min_samples_split : int or float, default=2\n"," |      The minimum number of samples required to split an internal node:\n"," |  \n"," |      - If int, then consider `min_samples_split` as the minimum number.\n"," |      - If float, then `min_samples_split` is a fraction and\n"," |        `ceil(min_samples_split * n_samples)` are the minimum\n"," |        number of samples for each split.\n"," |  \n"," |      .. versionchanged:: 0.18\n"," |         Added float values for fractions.\n"," |  \n"," |  min_samples_leaf : int or float, default=1\n"," |      The minimum number of samples required to be at a leaf node.\n"," |      A split point at any depth will only be considered if it leaves at\n"," |      least ``min_samples_leaf`` training samples in each of the left and\n"," |      right branches.  This may have the effect of smoothing the model,\n"," |      especially in regression.\n"," |  \n"," |      - If int, then consider `min_samples_leaf` as the minimum number.\n"," |      - If float, then `min_samples_leaf` is a fraction and\n"," |        `ceil(min_samples_leaf * n_samples)` are the minimum\n"," |        number of samples for each node.\n"," |  \n"," |      .. versionchanged:: 0.18\n"," |         Added float values for fractions.\n"," |  \n"," |  min_weight_fraction_leaf : float, default=0.0\n"," |      The minimum weighted fraction of the sum total of weights (of all\n"," |      the input samples) required to be at a leaf node. Samples have\n"," |      equal weight when sample_weight is not provided.\n"," |  \n"," |  max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n"," |      The number of features to consider when looking for the best split:\n"," |  \n"," |      - If int, then consider `max_features` features at each split.\n"," |      - If float, then `max_features` is a fraction and\n"," |        `max(1, int(max_features * n_features_in_))` features are considered at each\n"," |        split.\n"," |      - If \"auto\", then `max_features=sqrt(n_features)`.\n"," |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n"," |      - If \"log2\", then `max_features=log2(n_features)`.\n"," |      - If None, then `max_features=n_features`.\n"," |  \n"," |      .. versionchanged:: 1.1\n"," |          The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n"," |  \n"," |      .. deprecated:: 1.1\n"," |          The `\"auto\"` option was deprecated in 1.1 and will be removed\n"," |          in 1.3.\n"," |  \n"," |      Note: the search for a split does not stop until at least one\n"," |      valid partition of the node samples is found, even if it requires to\n"," |      effectively inspect more than ``max_features`` features.\n"," |  \n"," |  max_leaf_nodes : int, default=None\n"," |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n"," |      Best nodes are defined as relative reduction in impurity.\n"," |      If None then unlimited number of leaf nodes.\n"," |  \n"," |  min_impurity_decrease : float, default=0.0\n"," |      A node will be split if this split induces a decrease of the impurity\n"," |      greater than or equal to this value.\n"," |  \n"," |      The weighted impurity decrease equation is the following::\n"," |  \n"," |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n"," |                              - N_t_L / N_t * left_impurity)\n"," |  \n"," |      where ``N`` is the total number of samples, ``N_t`` is the number of\n"," |      samples at the current node, ``N_t_L`` is the number of samples in the\n"," |      left child, and ``N_t_R`` is the number of samples in the right child.\n"," |  \n"," |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n"," |      if ``sample_weight`` is passed.\n"," |  \n"," |      .. versionadded:: 0.19\n"," |  \n"," |  bootstrap : bool, default=True\n"," |      Whether bootstrap samples are used when building trees. If False, the\n"," |      whole dataset is used to build each tree.\n"," |  \n"," |  oob_score : bool, default=False\n"," |      Whether to use out-of-bag samples to estimate the generalization score.\n"," |      Only available if bootstrap=True.\n"," |  \n"," |  n_jobs : int, default=None\n"," |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n"," |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n"," |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n"," |      context. ``-1`` means using all processors. See :term:`Glossary\n"," |      <n_jobs>` for more details.\n"," |  \n"," |  random_state : int, RandomState instance or None, default=None\n"," |      Controls both the randomness of the bootstrapping of the samples used\n"," |      when building trees (if ``bootstrap=True``) and the sampling of the\n"," |      features to consider when looking for the best split at each node\n"," |      (if ``max_features < n_features``).\n"," |      See :term:`Glossary <random_state>` for details.\n"," |  \n"," |  verbose : int, default=0\n"," |      Controls the verbosity when fitting and predicting.\n"," |  \n"," |  warm_start : bool, default=False\n"," |      When set to ``True``, reuse the solution of the previous call to fit\n"," |      and add more estimators to the ensemble, otherwise, just fit a whole\n"," |      new forest. See :term:`Glossary <warm_start>` and\n"," |      :ref:`gradient_boosting_warm_start` for details.\n"," |  \n"," |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n"," |      Weights associated with classes in the form ``{class_label: weight}``.\n"," |      If not given, all classes are supposed to have weight one. For\n"," |      multi-output problems, a list of dicts can be provided in the same\n"," |      order as the columns of y.\n"," |  \n"," |      Note that for multioutput (including multilabel) weights should be\n"," |      defined for each class of every column in its own dict. For example,\n"," |      for four-class multilabel classification weights should be\n"," |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n"," |      [{1:1}, {2:5}, {3:1}, {4:1}].\n"," |  \n"," |      The \"balanced\" mode uses the values of y to automatically adjust\n"," |      weights inversely proportional to class frequencies in the input data\n"," |      as ``n_samples / (n_classes * np.bincount(y))``\n"," |  \n"," |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n"," |      weights are computed based on the bootstrap sample for every tree\n"," |      grown.\n"," |  \n"," |      For multi-output, the weights of each column of y will be multiplied.\n"," |  \n"," |      Note that these weights will be multiplied with sample_weight (passed\n"," |      through the fit method) if sample_weight is specified.\n"," |  \n"," |  ccp_alpha : non-negative float, default=0.0\n"," |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n"," |      subtree with the largest cost complexity that is smaller than\n"," |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n"," |      :ref:`minimal_cost_complexity_pruning` for details.\n"," |  \n"," |      .. versionadded:: 0.22\n"," |  \n"," |  max_samples : int or float, default=None\n"," |      If bootstrap is True, the number of samples to draw from X\n"," |      to train each base estimator.\n"," |  \n"," |      - If None (default), then draw `X.shape[0]` samples.\n"," |      - If int, then draw `max_samples` samples.\n"," |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n"," |        `max_samples` should be in the interval `(0.0, 1.0]`.\n"," |  \n"," |      .. versionadded:: 0.22\n"," |  \n"," |  Attributes\n"," |  ----------\n"," |  estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n"," |      The child estimator template used to create the collection of fitted\n"," |      sub-estimators.\n"," |  \n"," |      .. versionadded:: 1.2\n"," |         `base_estimator_` was renamed to `estimator_`.\n"," |  \n"," |  base_estimator_ : DecisionTreeClassifier\n"," |      The child estimator template used to create the collection of fitted\n"," |      sub-estimators.\n"," |  \n"," |      .. deprecated:: 1.2\n"," |          `base_estimator_` is deprecated and will be removed in 1.4.\n"," |          Use `estimator_` instead.\n"," |  \n"," |  estimators_ : list of DecisionTreeClassifier\n"," |      The collection of fitted sub-estimators.\n"," |  \n"," |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n"," |      The classes labels (single output problem), or a list of arrays of\n"," |      class labels (multi-output problem).\n"," |  \n"," |  n_classes_ : int or list\n"," |      The number of classes (single output problem), or a list containing the\n"," |      number of classes for each output (multi-output problem).\n"," |  \n"," |  n_features_in_ : int\n"," |      Number of features seen during :term:`fit`.\n"," |  \n"," |      .. versionadded:: 0.24\n"," |  \n"," |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n"," |      Names of features seen during :term:`fit`. Defined only when `X`\n"," |      has feature names that are all strings.\n"," |  \n"," |      .. versionadded:: 1.0\n"," |  \n"," |  n_outputs_ : int\n"," |      The number of outputs when ``fit`` is performed.\n"," |  \n"," |  feature_importances_ : ndarray of shape (n_features,)\n"," |      The impurity-based feature importances.\n"," |      The higher, the more important the feature.\n"," |      The importance of a feature is computed as the (normalized)\n"," |      total reduction of the criterion brought by that feature.  It is also\n"," |      known as the Gini importance.\n"," |  \n"," |      Warning: impurity-based feature importances can be misleading for\n"," |      high cardinality features (many unique values). See\n"," |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n"," |  \n"," |  oob_score_ : float\n"," |      Score of the training dataset obtained using an out-of-bag estimate.\n"," |      This attribute exists only when ``oob_score`` is True.\n"," |  \n"," |  oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n"," |      Decision function computed with out-of-bag estimate on the training\n"," |      set. If n_estimators is small it might be possible that a data point\n"," |      was never left out during the bootstrap. In this case,\n"," |      `oob_decision_function_` might contain NaN. This attribute exists\n"," |      only when ``oob_score`` is True.\n"," |  \n"," |  See Also\n"," |  --------\n"," |  sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n"," |  sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n"," |      tree classifiers.\n"," |  \n"," |  Notes\n"," |  -----\n"," |  The default values for the parameters controlling the size of the trees\n"," |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n"," |  unpruned trees which can potentially be very large on some data sets. To\n"," |  reduce memory consumption, the complexity and size of the trees should be\n"," |  controlled by setting those parameter values.\n"," |  \n"," |  The features are always randomly permuted at each split. Therefore,\n"," |  the best found split may vary, even with the same training data,\n"," |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n"," |  of the criterion is identical for several splits enumerated during the\n"," |  search of the best split. To obtain a deterministic behaviour during\n"," |  fitting, ``random_state`` has to be fixed.\n"," |  \n"," |  References\n"," |  ----------\n"," |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n"," |  \n"," |  Examples\n"," |  --------\n"," |  >>> from sklearn.ensemble import RandomForestClassifier\n"," |  >>> from sklearn.datasets import make_classification\n"," |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n"," |  ...                            n_informative=2, n_redundant=0,\n"," |  ...                            random_state=0, shuffle=False)\n"," |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n"," |  >>> clf.fit(X, y)\n"," |  RandomForestClassifier(...)\n"," |  >>> print(clf.predict([[0, 0, 0, 0]]))\n"," |  [1]\n"," |  \n"," |  Method resolution order:\n"," |      RandomForestClassifier\n"," |      ForestClassifier\n"," |      sklearn.base.ClassifierMixin\n"," |      BaseForest\n"," |      sklearn.base.MultiOutputMixin\n"," |      sklearn.ensemble._base.BaseEnsemble\n"," |      sklearn.base.MetaEstimatorMixin\n"," |      sklearn.base.BaseEstimator\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes defined here:\n"," |  \n"," |  __abstractmethods__ = frozenset()\n"," |  \n"," |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from ForestClassifier:\n"," |  \n"," |  predict(self, X)\n"," |      Predict class for X.\n"," |      \n"," |      The predicted class of an input sample is a vote by the trees in\n"," |      the forest, weighted by their probability estimates. That is,\n"," |      the predicted class is the one with highest mean probability\n"," |      estimate across the trees.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n"," |          The predicted classes.\n"," |  \n"," |  predict_log_proba(self, X)\n"," |      Predict class log-probabilities for X.\n"," |      \n"," |      The predicted class log-probabilities of an input sample is computed as\n"," |      the log of the mean predicted class probabilities of the trees in the\n"," |      forest.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n"," |          The class probabilities of the input samples. The order of the\n"," |          classes corresponds to that in the attribute :term:`classes_`.\n"," |  \n"," |  predict_proba(self, X)\n"," |      Predict class probabilities for X.\n"," |      \n"," |      The predicted class probabilities of an input sample are computed as\n"," |      the mean predicted class probabilities of the trees in the forest.\n"," |      The class probability of a single tree is the fraction of samples of\n"," |      the same class in a leaf.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n"," |          The class probabilities of the input samples. The order of the\n"," |          classes corresponds to that in the attribute :term:`classes_`.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.ClassifierMixin:\n"," |  \n"," |  score(self, X, y, sample_weight=None)\n"," |      Return the mean accuracy on the given test data and labels.\n"," |      \n"," |      In multi-label classification, this is the subset accuracy\n"," |      which is a harsh metric since you require for each sample that\n"," |      each label set be correctly predicted.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : array-like of shape (n_samples, n_features)\n"," |          Test samples.\n"," |      \n"," |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n"," |          True labels for `X`.\n"," |      \n"," |      sample_weight : array-like of shape (n_samples,), default=None\n"," |          Sample weights.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      score : float\n"," |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from BaseForest:\n"," |  \n"," |  apply(self, X)\n"," |      Apply trees in the forest to X, return leaf indices.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      X_leaves : ndarray of shape (n_samples, n_estimators)\n"," |          For each datapoint x in X and for each tree in the forest,\n"," |          return the index of the leaf x ends up in.\n"," |  \n"," |  decision_path(self, X)\n"," |      Return the decision path in the forest.\n"," |      \n"," |      .. versionadded:: 0.18\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      indicator : sparse matrix of shape (n_samples, n_nodes)\n"," |          Return a node indicator matrix where non zero elements indicates\n"," |          that the samples goes through the nodes. The matrix is of CSR\n"," |          format.\n"," |      \n"," |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n"," |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n"," |          gives the indicator value for the i-th estimator.\n"," |  \n"," |  fit(self, X, y, sample_weight=None)\n"," |      Build a forest of trees from the training set (X, y).\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The training input samples. Internally, its dtype will be converted\n"," |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csc_matrix``.\n"," |      \n"," |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n"," |          The target values (class labels in classification, real numbers in\n"," |          regression).\n"," |      \n"," |      sample_weight : array-like of shape (n_samples,), default=None\n"," |          Sample weights. If None, then samples are equally weighted. Splits\n"," |          that would create child nodes with net zero or negative weight are\n"," |          ignored while searching for a split in each node. In the case of\n"," |          classification, splits are also ignored if they would result in any\n"," |          single class carrying a negative weight in either child node.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : object\n"," |          Fitted estimator.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Readonly properties inherited from BaseForest:\n"," |  \n"," |  feature_importances_\n"," |      The impurity-based feature importances.\n"," |      \n"," |      The higher, the more important the feature.\n"," |      The importance of a feature is computed as the (normalized)\n"," |      total reduction of the criterion brought by that feature.  It is also\n"," |      known as the Gini importance.\n"," |      \n"," |      Warning: impurity-based feature importances can be misleading for\n"," |      high cardinality features (many unique values). See\n"," |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      feature_importances_ : ndarray of shape (n_features,)\n"," |          The values of this array sum to 1, unless all trees are single node\n"," |          trees consisting of only the root node, in which case it will be an\n"," |          array of zeros.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n"," |  \n"," |  __getitem__(self, index)\n"," |      Return the index'th estimator in the ensemble.\n"," |  \n"," |  __iter__(self)\n"," |      Return iterator over estimators in the ensemble.\n"," |  \n"," |  __len__(self)\n"," |      Return the number of estimators in the ensemble.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Readonly properties inherited from sklearn.ensemble._base.BaseEnsemble:\n"," |  \n"," |  base_estimator_\n"," |      Estimator used to grow the ensemble.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.BaseEstimator:\n"," |  \n"," |  __getstate__(self)\n"," |  \n"," |  __repr__(self, N_CHAR_MAX=700)\n"," |      Return repr(self).\n"," |  \n"," |  __setstate__(self, state)\n"," |  \n"," |  get_params(self, deep=True)\n"," |      Get parameters for this estimator.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      deep : bool, default=True\n"," |          If True, will return the parameters for this estimator and\n"," |          contained subobjects that are estimators.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      params : dict\n"," |          Parameter names mapped to their values.\n"," |  \n"," |  set_params(self, **params)\n"," |      Set the parameters of this estimator.\n"," |      \n"," |      The method works on simple estimators as well as on nested objects\n"," |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n"," |      parameters of the form ``<component>__<parameter>`` so that it's\n"," |      possible to update each component of a nested object.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      **params : dict\n"," |          Estimator parameters.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : estimator instance\n"," |          Estimator instance.\n","\n"]}]},{"cell_type":"code","source":["param_dist = {'max_depth':[3,None],                   #max depth - how much should decision tree comes - in what leafnode we should get output in what sample should we split\n","              'max_features':sp_randit(1,11),\n","              'min_samples_split':sp_randit(2,11),\n","              'criterion': [\"gini\",\"entropy\"]}"],"metadata":{"id":"Il7i8J9MjgKt","executionInfo":{"status":"ok","timestamp":1717998160588,"user_tz":-330,"elapsed":661,"user":{"displayName":"naimu samad","userId":"06085924539137588202"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["randomCV = RandomizedSearchCV(clf,param_distributions= param_dist,cv=3)"],"metadata":{"id":"eBYJeF9WlPQy","executionInfo":{"status":"ok","timestamp":1717998162045,"user_tz":-330,"elapsed":2,"user":{"displayName":"naimu samad","userId":"06085924539137588202"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["randomCV.fit(x_train,y_train)\n","print(randomCV.best_params_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XUIjxeL7lyoX","executionInfo":{"status":"ok","timestamp":1717998167821,"user_tz":-330,"elapsed":4171,"user":{"displayName":"naimu samad","userId":"06085924539137588202"}},"outputId":"65b68939-bdae-461e-dcb9-e3151d2013a5"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["{'criterion': 'entropy', 'max_depth': None, 'max_features': 7, 'min_samples_split': 7}\n"]}]},{"cell_type":"code","source":["print(randomCV.score(x_test,y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T0jmvFqQmLBO","executionInfo":{"status":"ok","timestamp":1717998167821,"user_tz":-330,"elapsed":19,"user":{"displayName":"naimu samad","userId":"06085924539137588202"}},"outputId":"4aff97fa-b8f4-428d-e475-d6c5e7440cb4"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["0.7916666666666666\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"HLnqvkw3ma1E","executionInfo":{"status":"ok","timestamp":1717997828452,"user_tz":-330,"elapsed":7,"user":{"displayName":"naimu samad","userId":"06085924539137588202"}}},"execution_count":19,"outputs":[]}]}